# Agentic AI Platform

Production-ready multi-agent AI platform with LangChain, MCP, and Couchbase integration.

## ğŸš€ Features

- ğŸ¤– **Multi-Agent Orchestration**: Specialized agents for different tasks
- ğŸ”Œ **MCP Integration**: Model Context Protocol for tool integration
- ğŸ“Š **MLflow Experiment Tracking**: Track agent performance, hyperparameters, and P99 latency
- ğŸ§ª **Comprehensive Testing**: 34+ tests covering edge cases, service failures, and ground truth
- ğŸ“ˆ **Production Monitoring**: Prometheus + Grafana with real-time dashboards
- ğŸ’¾ **Couchbase Integration**: Direct database querying via MCP tools
- âš¡ **Caching & Rate Limiting**: Performance optimization
- ğŸ¯ **Pipeline Performance Analysis**: Embedding vs LLM latency breakdown

## ğŸ“Š Production Monitoring (Prometheus + Grafana)

Real-time monitoring with custom metrics and latency heatmaps.

![Grafana Dashboard](docs/images/grafana-dashboard.png)

## ğŸ“Š MLflow Experiment Tracking

Track experiments, compare configurations, and monitor P99 latency.

![MLflow UI](docs/images/mlflow-ui.png)

**Features:**
- Request Latency Heatmap visualization
- Embedding vs LLM Generation Latency comparison
- Pipeline Stage Breakdown (know exactly which part is slow)
- Success Rate monitoring
- Latency Percentiles (P50, P95, P99)

**Quick Start:**
```bash
# Start all services (API, Prometheus, Grafana)
docker-compose up -d

# Generate traffic to populate metrics
python scripts/generate_monitoring_traffic.py 5

# Access Grafana
# http://localhost:3000 (admin/admin)
# Navigate to: Dashboards â†’ Agentic AI Platform - Production Monitoring
```

See [docs/PROMETHEUS_GRAFANA_SETUP.md](docs/PROMETHEUS_GRAFANA_SETUP.md) for detailed setup guide.

## ğŸ§ª Testing

Comprehensive test suite with 34+ tests covering:

- **Edge Cases**: Empty inputs, special characters, SQL injection attempts
- **Service Failures**: Couchbase 503, OpenAI 503, timeouts, Redis failures
- **Ground Truth**: 5 fixed Q&A pairs for smoke testing
- **API Endpoints**: Full FastAPI TestClient coverage

**Run Tests:**
```bash
# Run all tests
pytest tests/ -v

# Run by category
pytest tests/unit/test_edge_cases.py -v
pytest tests/unit/test_service_failures.py -v
pytest tests/integration/test_ground_truth.py -v
pytest tests/integration/test_api.py -v

# With coverage
pytest tests/ --cov=src/agentic_ai --cov-report=html
```

## ğŸ“Š MLflow Experiment Tracking

Track agent performance, compare configurations, and monitor P99 latency with MLflow.

![MLflow UI](docs/images/mlflow-ui.png)

**Quick Start:**
```bash
# Enable MLflow
export MLFLOW_ENABLED=true

# Run evaluation
python scripts/evaluate_agent.py \
    --queries "What hotels are in Paris?" "Show me routes from NYC to London" \
    --mlflow-enabled

# View results
mlflow ui
```

**Features:**
- Hyperparameter logging (model, temperature, max tokens)
- P99 latency tracking
- Success rate monitoring
- Model versioning
- Experiment comparison

See [docs/MLFLOW_USAGE.md](docs/MLFLOW_USAGE.md) for detailed guide.

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   FastAPI   â”‚
â”‚     API     â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”œâ”€â”€â–º Prometheus (Metrics)
       â”œâ”€â”€â–º Grafana (Visualization)
       â”œâ”€â”€â–º MLflow (Experiment Tracking)
       â”œâ”€â”€â–º Redis (Caching)
       â””â”€â”€â–º Couchbase (Database via MCP)
```

## ğŸš€ Quick Start

### Prerequisites

- Python 3.11+
- Docker & Docker Compose
- Couchbase connection (or use travel-sample bucket)

### Installation

```bash
# Clone repository
git clone https://github.com/Igosain08/Agentic-AI-Platform.git
cd Agentic-AI-Platform

# Install dependencies
pip install -e .[dev]

# Set up environment variables
cp .env.example .env
# Edit .env with your API keys and Couchbase credentials
```

### Running Locally

```bash
# Start all services
docker-compose up -d

# API will be available at http://localhost:8000
# Grafana at http://localhost:3000
# Prometheus at http://localhost:9090
```

### API Usage

```bash
# Health check
curl http://localhost:8000/api/v1/health

# Query endpoint
curl -X POST http://localhost:8000/api/v1/query \
  -H "Content-Type: application/json" \
  -d '{
    "message": "What hotels are in Paris?",
    "thread_id": "user-123"
  }'
```

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ src/agentic_ai/          # Main application code
â”‚   â”œâ”€â”€ agents/              # Agent implementations
â”‚   â”œâ”€â”€ api/                 # FastAPI routes
â”‚   â”œâ”€â”€ core/                # Core functionality
â”‚   â”œâ”€â”€ monitoring/          # MLflow, Prometheus, logging
â”‚   â””â”€â”€ utils/               # Cache, rate limiting
â”œâ”€â”€ tests/                   # Test suite
â”‚   â”œâ”€â”€ unit/               # Unit tests
â”‚   â””â”€â”€ integration/       # Integration tests
â”œâ”€â”€ scripts/                # Utility scripts
â”œâ”€â”€ prometheus/             # Prometheus configuration
â”œâ”€â”€ grafana/                # Grafana dashboards
â””â”€â”€ docs/                   # Documentation
```

## ğŸ¯ Key Metrics

### Production Monitoring
- **Request Latency**: P50, P95, P99 tracked in real-time
- **Pipeline Breakdown**: Embedding vs LLM Generation latency
- **Success Rate**: 100% system health monitoring
- **Request Rate**: Traffic patterns and load analysis

### MLflow Tracking
- **P99 Latency**: Optimized to 25.3s (temperature 0.7)
- **Hyperparameters**: Model, temperature, max tokens
- **Experiment Comparison**: A/B testing different configurations

## ğŸ“š Documentation

- [MLflow Usage Guide](docs/MLFLOW_USAGE.md)
- [Prometheus & Grafana Setup](docs/PROMETHEUS_GRAFANA_SETUP.md)
- [Docker Troubleshooting](docs/DOCKER_TROUBLESHOOTING.md)
- [Testing Guide](FOR_GEMINI_TESTING_SUMMARY.md)

## ğŸ› ï¸ Development

### Running Tests

```bash
# All tests
pytest tests/ -v

# Specific category
pytest tests/unit/ -v
pytest tests/integration/ -v

# With coverage
pytest tests/ --cov=src/agentic_ai --cov-report=html
```

### Code Quality

```bash
# Linting
ruff check src/ tests/

# Formatting
ruff format src/ tests/

# Type checking
mypy src/
```

## ğŸ“Š Monitoring & Observability

### Grafana Dashboard

Access the production monitoring dashboard:
- URL: http://localhost:3000
- Login: admin/admin
- Dashboard: "Agentic AI Platform - Production Monitoring"

**Dashboard Panels:**
1. Request Rate - Traffic patterns
2. Request Latency Heatmap - Visual latency distribution â­
3. Embedding vs LLM Latency - Pipeline breakdown
4. Pipeline Stage Breakdown - Time analysis
5. Success Rate - System health
6. Latency Percentiles - P50, P95, P99

### Prometheus Metrics

Access Prometheus:
- URL: http://localhost:9090
- Metrics endpoint: http://localhost:8000/metrics

**Key Metrics:**
- `http_requests_total` - Request count
- `http_request_duration_seconds` - Request latency
- `agentic_ai_embedding_latency_seconds` - Embedding/DB query time
- `agentic_ai_llm_generation_latency_seconds` - LLM call time
- `agentic_ai_pipeline_stage_duration_seconds` - Pipeline breakdown

## ğŸ“ What This Demonstrates

### Technical Skills
- âœ… **Container Orchestration**: Docker Compose with multiple services
- âœ… **Production Monitoring**: Prometheus + Grafana setup
- âœ… **Experiment Tracking**: MLflow integration
- âœ… **Comprehensive Testing**: 34+ tests with edge cases
- âœ… **Custom Metrics**: Pipeline performance breakdown
- âœ… **Real-time Observability**: Live monitoring dashboards

### Senior Engineer Moves
- âœ… **Pipeline Breakdown**: Know exactly which part is slow (Embedding vs LLM)
- âœ… **Heatmap Visualization**: Visual latency distribution
- âœ… **Production Thinking**: Real-time monitoring for 2 AM failures
- âœ… **Testing Best Practices**: Edge cases, service failures, ground truth

## ğŸ“ˆ Performance Insights

Based on monitoring data:
- **Embedding Latency**: ~990Âµs (very fast!)
- **LLM Generation Latency**: ~18.5s P99 (the bottleneck)
- **Total P99 Latency**: ~25-27s
- **Success Rate**: 100%

**Optimization Opportunities:**
- LLM generation time is the main bottleneck
- Consider model optimization, caching, or faster models

## ğŸ¤ Contributing

Contributions welcome! Please:
1. Fork the repository
2. Create a feature branch
3. Add tests for new features
4. Ensure all tests pass
5. Submit a pull request

## ğŸ“ License

See [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- LangChain for agent framework
- MCP (Model Context Protocol) for tool integration
- MLflow for experiment tracking
- Prometheus & Grafana for monitoring

---

**Built with â¤ï¸ for production-ready AI applications**
